#!/usr/bin/env python3
"""
ARC Prize 2025 - Advanced Pipeline Analysis

Detailed analysis of pipeline components and debugging
of hypothesis generation and verification.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path.cwd()))

from src.core.pipeline import ARCReasoningPipeline, PipelineConfig
from src.data.loader import ARCDataLoader
from src.core.types import Task
import json

def analyze_single_task(task: Task, pipeline: ARCReasoningPipeline):
    """Analyze a single task in detail"""
    print(f"\n" + "="*60)
    print(f"TASK ANALYSIS: {task.task_id}")
    print(f"="*60)
    
    # Show task structure
    print(f"Training pairs: {len(task.train_pairs)}")
    print(f"Test inputs: {len(task.test_inputs)}")
    
    for i, pair in enumerate(task.train_pairs):
        input_grid = pair['input']
        output_grid = pair['output']
        print(f"\nTraining Example {i+1}:")
        print(f"  Input shape: {len(input_grid)}x{len(input_grid[0]) if input_grid else 0}")
        print(f"  Output shape: {len(output_grid)}x{len(output_grid[0]) if output_grid else 0}")
        print(f"  Input: {input_grid}")
        print(f"  Output: {output_grid}")
    
    # Step-by-step pipeline analysis
    print(f"\n" + "-"*40)
    print("PIPELINE ANALYSIS")
    print("-"*40)
    
    # Phase 1: Preprocessing
    print("\nPhase 1: Preprocessing")
    preprocessed_scenes = pipeline._preprocess_task(task)
    print(f"  Generated {len(preprocessed_scenes)} scene representations")
    
    for i, scene in enumerate(preprocessed_scenes[:2]):  # Show first 2
        print(f"  Scene {i}: {scene.shape}, {len(scene.objects)} objects, {len(scene.spatial_relations)} relations")
    
    # Phase 2: Hypothesis Generation
    print("\nPhase 2: Hypothesis Generation")
    all_hypotheses = pipeline._generate_hypotheses(task, preprocessed_scenes)
    print(f"  Generated {len(all_hypotheses)} total hypotheses")
    
    for i, hypothesis in enumerate(all_hypotheses):
        print(f"  Hypothesis {i+1}:")
        print(f"    Generated by: {hypothesis.generated_by}")
        print(f"    Description: {hypothesis.description}")
        print(f"    Confidence: {hypothesis.confidence}")
        print(f"    Transformations: {len(hypothesis.transformations)}")
        for j, transform in enumerate(hypothesis.transformations):
            print(f"      Transform {j+1}: {transform.rule_type} (conf: {transform.confidence})")
    
    # Phase 3: Verification (manually)
    print("\nPhase 3: Verification Analysis")
    verification_results = pipeline.verifier.verify_hypotheses(all_hypotheses, task)
    
    for i, result in enumerate(verification_results):
        print(f"  Verification Result {i+1}:")
        print(f"    Score: {result.score:.4f}")
        print(f"    Accuracy: {result.accuracy:.4f}")
        print(f"    Consistency: {result.consistency:.4f}")
        print(f"    Complexity penalty: {result.complexity_penalty:.4f}")
        print(f"    Execution success rate: {result.execution_success_rate:.4f}")
        
        if 'execution_details' in result.metadata:
            details = result.metadata['execution_details']
            print(f"    Execution details: {len(details)} attempts")
            for detail in details[:2]:  # Show first 2
                print(f"      {detail}")
    
    return {
        'task_id': task.task_id,
        'preprocessing': len(preprocessed_scenes),
        'hypotheses': len(all_hypotheses),
        'verification_results': [(r.score, r.accuracy) for r in verification_results]
    }

def main():
    """Run advanced pipeline analysis"""
    print("ARC Prize 2025 - Advanced Pipeline Analysis")
    print("="*50)
    
    # Load test data
    loader = ARCDataLoader('data')
    tasks = loader.load_training_tasks()[:1]  # Analyze just 1 task in detail
    
    # Create pipeline
    config = PipelineConfig(
        enable_symbolic=True,
        enable_llm=False,
        enable_vision=False,
        max_hypotheses_per_component=5,
        verification_threshold=0.1  # Lower threshold to see more results
    )
    pipeline = ARCReasoningPipeline(config)
    
    analysis_results = []
    
    for task in tasks:
        result = analyze_single_task(task, pipeline)
        analysis_results.append(result)
    
    print(f"\n" + "="*60)
    print("SUMMARY")
    print("="*60)
    
    for result in analysis_results:
        print(f"Task {result['task_id']}:")
        print(f"  Scenes: {result['preprocessing']}")
        print(f"  Hypotheses: {result['hypotheses']}")
        print(f"  Best score: {max([s for s, a in result['verification_results']], default=0):.4f}")
        print(f"  Best accuracy: {max([a for s, a in result['verification_results']], default=0):.4f}")

if __name__ == "__main__":
    main()
